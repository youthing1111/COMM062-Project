{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deap\n",
      "  Downloading deap-1.4.1.tar.gz (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/anaconda3/envs/ds/lib/python3.11/site-packages (from deap) (1.26.4)\n",
      "Building wheels for collected packages: deap\n",
      "  Building wheel for deap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deap: filename=deap-1.4.1-cp311-cp311-macosx_11_0_arm64.whl size=103874 sha256=9b050cdee6ed42fa03f98c7ff0a33d6c15aee0635a25b67235bc22d9d1df5c75\n",
      "  Stored in directory: /Users/amansharma/Library/Caches/pip/wheels/f8/64/b8/65eacfbff3024ae2e2beb22e691d5c8abb89fbd863b8049b5f\n",
      "Successfully built deap\n",
      "Installing collected packages: deap\n",
      "Successfully installed deap-1.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install deap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sympy.combinatorics.graycode import bin_to_gray,gray_to_bin\n",
    "from keras.models import load_model\n",
    "import operator\n",
    "import random\n",
    "import numpy\n",
    "import math\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "import math\n",
    "from keras import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('Cifar10Model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IP2bit(b):\n",
    "    newInd_IP=[]\n",
    "    for index in range(len(b)):\n",
    "        chromosome = format(b[index], \"016b\")\n",
    "        chromosome = bin_to_gray(chromosome)\n",
    "        newInd_IP.extend(chromosome)\n",
    "    #newInd_IP = bin_to_gray(newInd_IP)\n",
    "    newInd_IP = [int(i) for i in newInd_IP]\n",
    "    return newInd_IP\n",
    "\n",
    "def chrom2real(c):\n",
    "    numOfBits = 16\n",
    "    indasstring=''.join(map(str, c))\n",
    "    degray=gray_to_bin(indasstring)\n",
    "    numasint=int(degray, 2) # convert to int from base 2 list\n",
    "    numinrange=-1+2.0*numasint/(2**numOfBits)\n",
    "    return numinrange\n",
    "\n",
    "def separatevariables(v):\n",
    "#     print(v)\n",
    "    numOfBits = 16\n",
    "    sep = []\n",
    "    for i in range (0,numOfBits*650,numOfBits):\n",
    "        sep.append(chrom2real(v[i:i+numOfBits]))\n",
    "    return sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds/lib/python3.11/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'FitnessMin' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "/opt/anaconda3/envs/ds/lib/python3.11/site-packages/deap/creator.py:185: RuntimeWarning: A class named 'Particle' has already been created and it will be overwritten. Consider deleting previous creation of that class or rename it.\n",
      "  warnings.warn(\"A class named '{0}' has already been created and it \"\n",
      "2024-11-21 12:17:38.159764: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09972\n",
      "0.14214\n",
      "gen\tevals\tavg      \tstd      \tmin    \tmax    \n",
      "0  \t50   \t0.0999492\t0.0223573\t0.03794\t0.14214\n",
      "0.14662\n",
      "1  \t50   \t0.101554 \t0.0163641\t0.0597 \t0.14662\n",
      "0.18062\n",
      "2  \t50   \t0.100147 \t0.0218435\t0.05072\t0.18062\n",
      "3  \t50   \t0.0980096\t0.0228445\t0.04742\t0.1646 \n",
      "4  \t50   \t0.100444 \t0.0221582\t0.04168\t0.15174\n",
      "5  \t50   \t0.0992772\t0.016056 \t0.05692\t0.14054\n",
      "6  \t50   \t0.102634 \t0.0175284\t0.0672 \t0.15052\n",
      "7  \t50   \t0.0991756\t0.0196444\t0.04146\t0.15006\n",
      "8  \t50   \t0.100094 \t0.024872 \t0.0471 \t0.1582 \n",
      "9  \t50   \t0.0941192\t0.0195146\t0.04342\t0.1403 \n",
      "10 \t50   \t0.0977524\t0.0219839\t0.03206\t0.17026\n",
      "11 \t50   \t0.0965684\t0.019424 \t0.04378\t0.14526\n",
      "12 \t50   \t0.095478 \t0.0184868\t0.0499 \t0.1395 \n",
      "13 \t50   \t0.0946944\t0.0143612\t0.06348\t0.13346\n",
      "14 \t50   \t0.102168 \t0.0161696\t0.0605 \t0.1383 \n",
      "15 \t50   \t0.0926028\t0.0206951\t0.04704\t0.1392 \n",
      "16 \t50   \t0.0971976\t0.0205254\t0.04614\t0.1581 \n",
      "17 \t50   \t0.0984892\t0.0204452\t0.0487 \t0.1638 \n",
      "18 \t50   \t0.0988632\t0.0207239\t0.02928\t0.14922\n",
      "19 \t50   \t0.0962708\t0.0184201\t0.0387 \t0.14294\n",
      "20 \t50   \t0.0964748\t0.0208678\t0.02544\t0.16014\n",
      "21 \t50   \t0.0968704\t0.0166657\t0.05664\t0.13098\n",
      "22 \t50   \t0.0973088\t0.0189741\t0.05378\t0.15734\n",
      "23 \t50   \t0.101453 \t0.0188231\t0.04916\t0.15978\n",
      "24 \t50   \t0.093104 \t0.0159264\t0.05976\t0.13862\n",
      "0.19632\n",
      "25 \t50   \t0.104967 \t0.0251919\t0.05786\t0.19632\n",
      "26 \t50   \t0.0974088\t0.0212063\t0.0202 \t0.1405 \n",
      "27 \t50   \t0.097822 \t0.0193328\t0.0594 \t0.15296\n",
      "28 \t50   \t0.102377 \t0.0244356\t0.03338\t0.17464\n",
      "29 \t50   \t0.100022 \t0.0198329\t0.04776\t0.15922\n",
      "30 \t50   \t0.0972228\t0.0231061\t0.02222\t0.16714\n",
      "31 \t50   \t0.100467 \t0.0206365\t0.051  \t0.17848\n",
      "32 \t50   \t0.0980872\t0.0203326\t0.0363 \t0.14708\n",
      "33 \t50   \t0.095086 \t0.0221126\t0.0241 \t0.14736\n",
      "34 \t50   \t0.103473 \t0.0222725\t0.05196\t0.17354\n",
      "35 \t50   \t0.0933152\t0.0236803\t0.01338\t0.14392\n",
      "36 \t50   \t0.100695 \t0.0281104\t0.04158\t0.18106\n",
      "37 \t50   \t0.102411 \t0.0187084\t0.05868\t0.14798\n",
      "38 \t50   \t0.101588 \t0.0197436\t0.05204\t0.16372\n",
      "39 \t50   \t0.101036 \t0.0235402\t0.04512\t0.16164\n",
      "40 \t50   \t0.0999156\t0.0211431\t0.05216\t0.18952\n",
      "41 \t50   \t0.0956368\t0.0190176\t0.04538\t0.1407 \n",
      "42 \t50   \t0.0913788\t0.0243675\t0.03134\t0.12446\n",
      "43 \t50   \t0.097742 \t0.0188432\t0.0442 \t0.16218\n",
      "44 \t50   \t0.0946512\t0.0202387\t0.03752\t0.13792\n",
      "45 \t50   \t0.0924832\t0.0200879\t0.0442 \t0.14276\n",
      "46 \t50   \t0.0980692\t0.0215999\t0.04348\t0.1688 \n",
      "47 \t50   \t0.099758 \t0.0226187\t0.03712\t0.17418\n",
      "48 \t50   \t0.101018 \t0.0190688\t0.06002\t0.14536\n",
      "49 \t50   \t0.0990616\t0.016282 \t0.05576\t0.13248\n",
      "50 \t50   \t0.0966048\t0.0196886\t0.05394\t0.16142\n",
      "51 \t50   \t0.0967828\t0.0204195\t0.05056\t0.1479 \n",
      "52 \t50   \t0.0978284\t0.0207432\t0.02662\t0.15428\n",
      "53 \t50   \t0.0941232\t0.0174857\t0.04802\t0.12814\n",
      "54 \t50   \t0.0983856\t0.0190042\t0.03182\t0.1665 \n",
      "55 \t50   \t0.0991856\t0.01647  \t0.0499 \t0.15724\n",
      "56 \t50   \t0.1013   \t0.0188386\t0.05414\t0.15594\n",
      "57 \t50   \t0.098108 \t0.0180502\t0.04022\t0.15658\n",
      "58 \t50   \t0.0981756\t0.0212138\t0.04376\t0.16544\n",
      "59 \t50   \t0.102736 \t0.0172107\t0.0468 \t0.1493 \n",
      "60 \t50   \t0.099608 \t0.0184696\t0.0441 \t0.15432\n",
      "61 \t50   \t0.101484 \t0.0166617\t0.04786\t0.14576\n",
      "62 \t50   \t0.104134 \t0.0202482\t0.04866\t0.14832\n",
      "63 \t50   \t0.10378  \t0.0170377\t0.05352\t0.14054\n",
      "64 \t50   \t0.100571 \t0.0182798\t0.0501 \t0.14246\n",
      "0.20478\n",
      "65 \t50   \t0.104014 \t0.0262647\t0.04454\t0.20478\n",
      "66 \t50   \t0.09711  \t0.0225743\t0.0348 \t0.16328\n",
      "67 \t50   \t0.0971444\t0.0236895\t0.05096\t0.17044\n",
      "68 \t50   \t0.0972764\t0.0250722\t0.05394\t0.19378\n",
      "69 \t50   \t0.0988952\t0.0276983\t0.03748\t0.1938 \n",
      "70 \t50   \t0.102371 \t0.0282422\t0.04434\t0.17016\n",
      "71 \t50   \t0.103065 \t0.0248928\t0.05504\t0.19384\n",
      "72 \t50   \t0.0999464\t0.0252867\t0.02448\t0.20238\n",
      "73 \t50   \t0.100904 \t0.024355 \t0.04516\t0.19934\n",
      "74 \t50   \t0.0995072\t0.0267124\t0.03356\t0.19434\n",
      "75 \t50   \t0.0974128\t0.0246689\t0.03516\t0.18718\n",
      "76 \t50   \t0.105066 \t0.0215192\t0.06098\t0.20292\n",
      "77 \t50   \t0.0967644\t0.0269568\t0.02636\t0.1841 \n",
      "0.20676\n",
      "78 \t50   \t0.103776 \t0.0300814\t0.0433 \t0.20676\n",
      "79 \t50   \t0.104918 \t0.0256228\t0.05018\t0.20584\n",
      "0.20724\n",
      "80 \t50   \t0.0939564\t0.0259351\t0.02576\t0.20724\n",
      "81 \t50   \t0.100054 \t0.026925 \t0.0295 \t0.20274\n",
      "82 \t50   \t0.103237 \t0.0249739\t0.04816\t0.20274\n",
      "83 \t50   \t0.102682 \t0.0231698\t0.054  \t0.2056 \n",
      "0.2074\n",
      "84 \t50   \t0.0986484\t0.0279569\t0.04732\t0.2074 \n",
      "85 \t50   \t0.101867 \t0.0215883\t0.06434\t0.20312\n",
      "86 \t50   \t0.106017 \t0.0237339\t0.05978\t0.20724\n",
      "87 \t50   \t0.103648 \t0.0246269\t0.054  \t0.20724\n",
      "0.20754\n",
      "88 \t50   \t0.100635 \t0.0285341\t0.04542\t0.20754\n",
      "0.20762\n",
      "89 \t50   \t0.0983716\t0.0277994\t0.0326 \t0.20762\n",
      "90 \t50   \t0.10318  \t0.024658 \t0.06282\t0.20762\n",
      "91 \t50   \t0.101152 \t0.0256871\t0.04294\t0.20762\n",
      "92 \t50   \t0.103255 \t0.0264919\t0.02538\t0.20762\n",
      "93 \t50   \t0.105199 \t0.0301561\t0.05846\t0.20762\n",
      "94 \t50   \t0.101602 \t0.0229756\t0.05638\t0.20762\n",
      "95 \t50   \t0.102653 \t0.0233677\t0.05848\t0.20762\n",
      "96 \t50   \t0.0938212\t0.0265758\t0.03962\t0.20762\n",
      "97 \t50   \t0.101482 \t0.0244506\t0.04912\t0.20762\n",
      "98 \t50   \t0.100277 \t0.0232444\t0.05458\t0.20762\n",
      "99 \t50   \t0.099452 \t0.0229318\t0.04892\t0.20762\n",
      "100\t50   \t0.101192 \t0.0252942\t0.05078\t0.20762\n",
      "101\t50   \t0.0988508\t0.0278961\t0.0344 \t0.20762\n",
      "102\t50   \t0.103212 \t0.027125 \t0.03406\t0.20762\n",
      "103\t50   \t0.100144 \t0.0264805\t0.03358\t0.20762\n",
      "104\t50   \t0.103245 \t0.023917 \t0.0448 \t0.20762\n",
      "105\t50   \t0.103764 \t0.0262486\t0.063  \t0.20762\n",
      "106\t50   \t0.0994212\t0.0245801\t0.05238\t0.20762\n",
      "107\t50   \t0.0984668\t0.0289475\t0.04204\t0.20762\n",
      "108\t50   \t0.107354 \t0.025233 \t0.0531 \t0.20762\n",
      "109\t50   \t0.102166 \t0.0201843\t0.0568 \t0.20762\n",
      "110\t50   \t0.101074 \t0.0274016\t0.04764\t0.20762\n",
      "111\t50   \t0.103586 \t0.0212965\t0.06322\t0.20762\n",
      "112\t50   \t0.097724 \t0.023264 \t0.0446 \t0.20762\n",
      "113\t50   \t0.103066 \t0.0218104\t0.0373 \t0.20762\n",
      "114\t50   \t0.0989136\t0.026113 \t0.03472\t0.20762\n",
      "115\t50   \t0.103471 \t0.0248582\t0.06122\t0.20762\n",
      "116\t50   \t0.0973792\t0.0312572\t0.03502\t0.20762\n",
      "117\t50   \t0.107342 \t0.0292693\t0.0643 \t0.20762\n",
      "118\t50   \t0.104136 \t0.0205703\t0.06184\t0.20762\n",
      "119\t50   \t0.0988808\t0.0261136\t0.03952\t0.20762\n",
      "120\t50   \t0.105789 \t0.0270818\t0.02834\t0.20762\n",
      "121\t50   \t0.0984628\t0.0284789\t0.03894\t0.20762\n",
      "122\t50   \t0.096816 \t0.025736 \t0.04648\t0.20762\n",
      "123\t50   \t0.102668 \t0.024125 \t0.04732\t0.20762\n",
      "124\t50   \t0.0993732\t0.0265417\t0.04752\t0.20762\n",
      "125\t50   \t0.103101 \t0.0243931\t0.0403 \t0.20762\n",
      "126\t50   \t0.109832 \t0.0275075\t0.06356\t0.20762\n",
      "127\t50   \t0.0999028\t0.0243788\t0.04216\t0.20762\n",
      "128\t50   \t0.0997096\t0.0216651\t0.0602 \t0.20762\n",
      "129\t50   \t0.098126 \t0.0209766\t0.0544 \t0.20762\n",
      "130\t50   \t0.109037 \t0.0259455\t0.04418\t0.20762\n",
      "131\t50   \t0.0988224\t0.0255953\t0.04366\t0.20762\n",
      "132\t50   \t0.0987152\t0.0237456\t0.04622\t0.20762\n",
      "133\t50   \t0.0998576\t0.0244196\t0.05282\t0.20762\n",
      "134\t50   \t0.103916 \t0.0259503\t0.03588\t0.20762\n",
      "135\t50   \t0.107778 \t0.027444 \t0.05846\t0.20762\n",
      "136\t50   \t0.104889 \t0.0264985\t0.05062\t0.20762\n",
      "137\t50   \t0.096736 \t0.0270443\t0.03536\t0.20762\n",
      "138\t50   \t0.0994516\t0.0246407\t0.05122\t0.20762\n",
      "139\t50   \t0.102153 \t0.0268505\t0.03908\t0.20762\n",
      "140\t50   \t0.10723  \t0.0274048\t0.0463 \t0.20762\n",
      "141\t50   \t0.101314 \t0.0281645\t0.03554\t0.20762\n",
      "142\t50   \t0.102598 \t0.0248466\t0.03846\t0.20762\n",
      "143\t50   \t0.101874 \t0.0253089\t0.05774\t0.20762\n",
      "144\t50   \t0.105856 \t0.0234611\t0.056  \t0.20762\n",
      "145\t50   \t0.100868 \t0.0222934\t0.04906\t0.20762\n",
      "146\t50   \t0.109756 \t0.0273229\t0.0383 \t0.20762\n",
      "147\t50   \t0.0986756\t0.0236714\t0.0364 \t0.20762\n",
      "148\t50   \t0.101495 \t0.0242761\t0.05068\t0.20762\n",
      "149\t50   \t0.100822 \t0.0254332\t0.03776\t0.20762\n",
      "150\t50   \t0.103256 \t0.0274548\t0.04638\t0.20762\n",
      "151\t50   \t0.101709 \t0.0237067\t0.06414\t0.20762\n",
      "152\t50   \t0.0993136\t0.0246564\t0.04058\t0.20762\n",
      "153\t50   \t0.104742 \t0.0292064\t0.04948\t0.20762\n",
      "154\t50   \t0.104989 \t0.0243236\t0.0709 \t0.20762\n",
      "155\t50   \t0.101044 \t0.0238886\t0.04134\t0.20762\n",
      "156\t50   \t0.101615 \t0.0275338\t0.04958\t0.20762\n",
      "157\t50   \t0.1004   \t0.0248077\t0.05236\t0.20762\n",
      "158\t50   \t0.103816 \t0.0286391\t0.0556 \t0.20762\n",
      "159\t50   \t0.103294 \t0.0241066\t0.05654\t0.20762\n",
      "160\t50   \t0.107945 \t0.0296169\t0.0275 \t0.20762\n",
      "161\t50   \t0.101549 \t0.0255771\t0.05008\t0.20762\n",
      "162\t50   \t0.102902 \t0.0235892\t0.05586\t0.20762\n",
      "163\t50   \t0.103502 \t0.0254782\t0.05608\t0.20762\n",
      "164\t50   \t0.105779 \t0.0301957\t0.03364\t0.20762\n",
      "165\t50   \t0.0994836\t0.0231164\t0.04284\t0.20762\n",
      "166\t50   \t0.10352  \t0.0239575\t0.04754\t0.20762\n",
      "167\t50   \t0.101681 \t0.0259944\t0.0485 \t0.20762\n",
      "168\t50   \t0.0975192\t0.0245966\t0.04412\t0.20762\n",
      "169\t50   \t0.100472 \t0.0245718\t0.04538\t0.20762\n",
      "170\t50   \t0.104951 \t0.0271896\t0.05384\t0.20762\n",
      "171\t50   \t0.100804 \t0.026611 \t0.04346\t0.20762\n",
      "172\t50   \t0.101477 \t0.0220646\t0.04602\t0.20762\n",
      "173\t50   \t0.102206 \t0.0291314\t0.03624\t0.20762\n",
      "174\t50   \t0.104063 \t0.0217834\t0.06666\t0.20762\n",
      "175\t50   \t0.102683 \t0.022828 \t0.06514\t0.20762\n",
      "176\t50   \t0.100885 \t0.0269182\t0.04668\t0.20762\n",
      "177\t50   \t0.10244  \t0.0263933\t0.03054\t0.20762\n",
      "178\t50   \t0.101744 \t0.0254156\t0.05346\t0.20762\n",
      "179\t50   \t0.10293  \t0.0239432\t0.04144\t0.20762\n",
      "180\t50   \t0.0993976\t0.0249606\t0.03812\t0.20762\n",
      "181\t50   \t0.0986884\t0.0230783\t0.05096\t0.20762\n",
      "182\t50   \t0.103426 \t0.0261875\t0.04846\t0.20762\n",
      "183\t50   \t0.0992912\t0.024411 \t0.05268\t0.20762\n",
      "184\t50   \t0.100691 \t0.0239294\t0.0461 \t0.20762\n",
      "185\t50   \t0.0998136\t0.028103 \t0.03896\t0.20762\n",
      "186\t50   \t0.101493 \t0.0225028\t0.04998\t0.20762\n",
      "187\t50   \t0.102675 \t0.0248126\t0.05476\t0.20762\n",
      "188\t50   \t0.10042  \t0.0247735\t0.04742\t0.20762\n",
      "189\t50   \t0.0962736\t0.0284675\t0.01894\t0.20762\n",
      "190\t50   \t0.102065 \t0.0255568\t0.0465 \t0.20762\n",
      "191\t50   \t0.0969668\t0.0244128\t0.04332\t0.20762\n",
      "192\t50   \t0.102616 \t0.0258349\t0.04108\t0.20762\n",
      "193\t50   \t0.10152  \t0.0213791\t0.0634 \t0.20762\n",
      "194\t50   \t0.0944856\t0.0225416\t0.0449 \t0.20762\n",
      "195\t50   \t0.103593 \t0.0238527\t0.0501 \t0.20762\n",
      "196\t50   \t0.101058 \t0.0228799\t0.05812\t0.20762\n",
      "197\t50   \t0.0976292\t0.0245321\t0.04774\t0.20762\n",
      "198\t50   \t0.0999144\t0.0256408\t0.03586\t0.20762\n",
      "199\t50   \t0.100308 \t0.0285282\t0.0478 \t0.20762\n",
      "best particle position is  [52478, 20878, 49763, 33252, 36714, 13602, 46618, 42433, 25806, 46699, 39499, 29305, 1, 25395, 52190, 15372, 34473, 3387, 16578, 24633, 30485, 26577, 49975, 19926, 33467, 51347, 49411, 60206, 4594, 22438, 17081, 49958, 17841, 65536, 21235, 3334, 15080, 39843, 24753, 47801, 24568, 25272, 38441, 37787, 38369, 21469, 16461, 21782, 143, 27721, 61759, 10460, 54249, 46406, 34542, 52230, 43263, 10483, 65536, 32582, 19495, 28324, 34313, 28274, 17112, 1, 55160, 30926, 55381, 49751, 29509, 38379, 39091, 57937, 21851, 18106, 50080, 43738, 23164, 5305, 27274, 17032, 25710, 50114, 33524, 27623, 22846, 28386, 29084, 40174, 15338, 36265, 46823, 23944, 1, 46520, 30015, 32113, 19478, 21707, 54066, 31739, 1148, 43228, 22796, 20933, 27865, 9549, 37699, 47752, 62941, 42203, 34303, 52802, 51423, 1, 65066, 12528, 9601, 38702, 63864, 32673, 65536, 57496, 61381, 61399, 16112, 32108, 30304, 39487, 44026, 11982, 32013, 1, 24780, 27131, 27625, 57185, 48883, 36768, 31511, 9776, 33988, 39937, 54125, 25479, 40482, 7364, 58803, 17656, 12013, 52926, 34682, 5894, 40325, 17394, 15126, 48812, 8171, 28045, 10, 6266, 20689, 36401, 31141, 31004, 1, 36487, 47987, 44619, 31535, 46722, 49139, 26247, 33278, 21190, 57670, 7360, 34161, 13387, 54642, 52603, 34444, 30035, 35086, 30877, 12481, 49206, 30530, 35673, 33130, 3663, 24806, 44321, 26444, 38346, 29990, 41424, 53610, 16609, 31232, 51457, 25393, 36554, 43181, 22383, 41203, 37761, 43533, 42943, 54954, 48209, 4431, 52823, 32218, 42403, 40878, 50700, 46339, 15494, 20993, 35419, 4031, 24889, 26243, 26292, 57734, 43617, 19738, 43162, 60441, 1, 34231, 2595, 39441, 48275, 18215, 819, 64429, 1, 28962, 29749, 24842, 9324, 27884, 41469, 9576, 23638, 36537, 7018, 8572, 22142, 13023, 35871, 56786, 52846, 20103, 36607, 44656, 41057, 16641, 35372, 35939, 14941, 2094, 44329, 42624, 26762, 40371, 29429, 23818, 47987, 28830, 46732, 18333, 21775, 26547, 15042, 16553, 62353, 21241, 6119, 18076, 31459, 34512, 64861, 38391, 36858, 45276, 43500, 23071, 34901, 17595, 37541, 30874, 34518, 11798, 31055, 34595, 23948, 36745, 38720, 27656, 45970, 25590, 13706, 16807, 19451, 34584, 37324, 48376, 41485, 16928, 61865, 6621, 42211, 31408, 11094, 33086, 38662, 56443, 24096, 25101, 6272, 39134, 33897, 39710, 38319, 17455, 13115, 53925, 5182, 40255, 44483, 1, 23338, 35601, 57073, 11823, 51611, 16865, 56385, 8512, 23056, 26915, 7879, 40145, 2048, 36381, 8513, 2029, 27134, 12538, 26407, 33323, 3166, 11737, 43216, 33230, 18637, 50665, 10293, 15463, 27229, 6392, 8909, 25811, 26030, 33142, 23594, 43227, 3119, 62862, 64074, 49225, 58311, 65536, 21782, 16554, 36481, 15415, 38509, 14273, 24496, 2168, 28108, 37144, 6580, 34692, 17293, 48811, 34896, 1, 9852, 44958, 1, 40839, 20002, 18024, 4291, 34634, 1, 2641, 39036, 23288, 9586, 40154, 27402, 12218, 19209, 44649, 38999, 4713, 20042, 53279, 62717, 47768, 27648, 41572, 61975, 34552, 331, 36998, 20340, 7153, 28867, 26316, 37963, 65536, 9964, 65536, 25414, 53636, 19151, 14333, 15165, 47034, 32207, 63466, 62676, 51206, 16423, 50638, 55504, 36770, 29934, 39132, 46847, 52409, 17985, 29344, 29479, 24190, 36228, 65536, 7002, 14493, 35393, 27652, 53640, 65536, 44808, 18000, 1, 37679, 6870, 36868, 34878, 39466, 29991, 41354, 36338, 58734, 41148, 40468, 32054, 50362, 39673, 34641, 51031, 43884, 63164, 1, 53027, 36028, 1, 44890, 20137, 8912, 49517, 54632, 37481, 21343, 23857, 49841, 1, 7433, 23707, 45655, 46860, 34219, 6632, 60568, 58250, 12828, 49125, 15129, 21171, 62737, 34612, 24805, 18580, 30176, 18994, 5211, 30593, 34565, 39623, 17030, 1, 12285, 35981, 64415, 60290, 35145, 23399, 46447, 2430, 51508, 49446, 37383, 27945, 29249, 35830, 18943, 14120, 40088, 24729, 51156, 1, 7496, 19087, 50340, 16051, 18617, 34774, 46719, 38497, 31816, 51161, 1, 46960, 8363, 29594, 62086, 21606, 47237, 50536, 29196, 43237, 19926, 64857, 46298, 19671, 54795, 26491, 1, 1, 19109, 16855, 31267, 42388, 18794, 43395, 55511, 87, 28830, 36247, 13943, 39701, 41491, 26838, 26984, 46742, 50198, 30989, 45000, 27390, 16835, 16910, 11014, 23922, 63848, 22552, 48504, 42309, 22327, 54373, 37211, 22106, 33031, 27080, 18894, 26897, 55279, 63160, 27400, 50850, 25847, 51833, 38889, 39116, 29949, 65536, 22020, 12517, 50284, 45423, 28220, 23739, 46807, 58059, 33348, 19236, 28902, 47851, 38396, 34468, 57696, 41432, 14217, 63135, 41106, 56855, 62679, 56699, 59659, 13620, 10687, 27290, 49895, 20589, 48431, 13133, 8143, 33740, 44990, 27205, 25219, 33718]\n",
      "(0.20762,)\n"
     ]
    }
   ],
   "source": [
    "# Sample code for PSO, using DEAP library\n",
    "\n",
    "numOfBits = 16\n",
    "Chrom_length = 650\n",
    "bit = 16\n",
    "posMinInit      = 1\n",
    "posMaxInit      = 2**bit\n",
    "VMaxInit        = 0.1*posMaxInit\n",
    "VMinInit        = -0.1*posMaxInit\n",
    "populationSize  = 50\n",
    "dimension       = int((Chrom_length*numOfBits)/bit)n\n",
    "interval        = 1\n",
    "iterations      = 200\n",
    "xmax = 50\n",
    "best_value = []\n",
    "fitness_list_best = []\n",
    "fitness_list_worst = []\n",
    "r = 0.018\n",
    "#Parameter setup\n",
    "\n",
    "wmax = 0.9 #weighting\n",
    "wmin = 0.4 \n",
    "c1   = 2.0\n",
    "c2   = 2.0\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(1.0,)) # -1 is for minimise\n",
    "creator.create(\"Particle\", list, fitness=creator.FitnessMin, speed=list, smin=None, smax=None, best=None)\n",
    "# particle rerpresented by list of 5 things\n",
    "# 1. fitness of the particle, \n",
    "# 2. speed of the particle which is also going to be a list, \n",
    "# 3.4. limit of the speed value, \n",
    "# 5. best state the particle has been in so far.\n",
    "\n",
    "def generate(size, smin, smax):\n",
    "    part = creator.Particle(int(random.uniform(posMinInit, posMaxInit)) for _ in range(size)) \n",
    "    part.speed = [random.uniform(VMinInit, VMaxInit) for _ in range(size)]\n",
    "    part.smin = smin \n",
    "    part.smax = smax\n",
    "    return part\n",
    "\n",
    "def updateParticle(part, best, weight,pop):\n",
    "\n",
    "    r1 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "    r2 = (random.uniform(0, 1) for _ in range(len(part)))\n",
    "\n",
    "    v_r0 = [weight*x for x in part.speed]\n",
    "    v_r1 = [c1*x for x in map(operator.mul, r1, map(operator.sub, part.best, part))] # local best\n",
    "    v_r2 = [c2*x for x in map(operator.mul, r2, map(operator.sub, best, part))] # global best\n",
    "    \n",
    "    part.speed = [0.7*x for x in map(operator.add, v_r0, map(operator.add, v_r1, v_r2))]\n",
    "\n",
    "    part[:] = list(map(operator.add, part, part.speed))\n",
    "    part[:] = [1 if i<1 else i for i in part[:]]\n",
    "    part[:] = [posMaxInit if i>posMaxInit else i for i in part[:]]\n",
    "    part[:] = [int(i) for i in part[:]]\n",
    "\n",
    "def loss_func(part):\n",
    "    new_pop = IP2bit(part)\n",
    "    layer_weights = separatevariables(new_pop)\n",
    "\n",
    "    weights = np.array(layer_weights[0:640]).reshape(64,10)\n",
    "    bias = np.array(layer_weights[640:]).reshape(10,)\n",
    "    layer_weights = [weights,bias]\n",
    "    model.layers[-1].set_weights(layer_weights)\n",
    "\n",
    "    y_pred = model.predict(X_train,verbose = 0)\n",
    "    y_pred_classes = [np.argmax(element) for element in y_pred]\n",
    "    loss = accuracy_score(y_train,y_pred_classes)\n",
    "    return loss,\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"particle\", generate, size=dimension, smin=-0.1*posMaxInit, smax=0.1*posMaxInit)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.particle)\n",
    "toolbox.register(\"update\", updateParticle)\n",
    "toolbox.register(\"evaluate\", loss_func) \n",
    "\n",
    "def main():\n",
    "    pop = toolbox.population(n=populationSize) # Population Size\n",
    "\n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", numpy.mean)\n",
    "    stats.register(\"std\", numpy.std)\n",
    "    stats.register(\"min\", numpy.min)\n",
    "    stats.register(\"max\", numpy.max)\n",
    "\n",
    "    logbook = tools.Logbook()\n",
    "    logbook.header = [\"gen\", \"evals\"] + stats.fields\n",
    "\n",
    "    best = None\n",
    "\n",
    "    #begin main loop\n",
    "    for g in range(iterations):\n",
    "        w = wmax - (wmax-wmin)*g/iterations #decaying inertia weight\n",
    "        for part in pop:\n",
    "            part.fitness.values = toolbox.evaluate(part) #actually only one fitness value\n",
    "        \n",
    "            #update local best\n",
    "            if (not part.best) or (part.best.fitness < part.fitness):   \n",
    "            #   best is None   or  current value is better              #< is overloaded        \n",
    "                part.best = creator.Particle(part)\n",
    "                part.best.fitness.values = part.fitness.values\n",
    "            \n",
    "            #update global best\n",
    "            if (not best) or best.fitness < part.fitness:\n",
    "                best = creator.Particle(part)\n",
    "                best.fitness.values = part.fitness.values\n",
    "                print(best.fitness.values[0])\n",
    "        for part in pop:\n",
    "            toolbox.update(part, best,w,pop)\n",
    "\n",
    "        sorted_pop = sorted(pop, key=lambda ind: ind.fitness.values, reverse=True)\n",
    "        fitness_list_best.append(sorted_pop[0].fitness.values[0])\n",
    "        fitness_list_worst.append(sorted_pop[-1].fitness.values[0])\n",
    "        # Gather all the fitnesses in one list and print the stats\n",
    "\n",
    "        if g%interval==0: # interval \n",
    "            logbook.record(gen=g, evals=len(pop), **stats.compile(pop))\n",
    "            print(logbook.stream)\n",
    "            #print('best ',best, best.fitness)\n",
    "    \n",
    "    print('best particle position is ',best)\n",
    "    print(best.fitness)\n",
    "    best_value.extend(best)\n",
    "    return pop, logbook, best,best_value,fitness_list_best,fitness_list_worst\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2053"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pop = IP2bit(best_value)\n",
    "layer_weights = separatevariables(new_pop)\n",
    "weights = np.array(layer_weights[0:640]).reshape(64,10)\n",
    "bias = np.array(layer_weights[640:]).reshape(10,)\n",
    "layer_weights = [weights,bias]\n",
    "model.layers[-1].set_weights(layer_weights)\n",
    "\n",
    "y_pred = model.predict(X_test,verbose = 0)\n",
    "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
    "loss = accuracy_score(y_test,y_pred_classes)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14214,\n",
       " 0.14662,\n",
       " 0.18062,\n",
       " 0.1646,\n",
       " 0.15174,\n",
       " 0.14054,\n",
       " 0.15052,\n",
       " 0.15006,\n",
       " 0.1582,\n",
       " 0.1403,\n",
       " 0.17026,\n",
       " 0.14526,\n",
       " 0.1395,\n",
       " 0.13346,\n",
       " 0.1383,\n",
       " 0.1392,\n",
       " 0.1581,\n",
       " 0.1638,\n",
       " 0.14922,\n",
       " 0.14294,\n",
       " 0.16014,\n",
       " 0.13098,\n",
       " 0.15734,\n",
       " 0.15978,\n",
       " 0.13862,\n",
       " 0.19632,\n",
       " 0.1405,\n",
       " 0.15296,\n",
       " 0.17464,\n",
       " 0.15922,\n",
       " 0.16714,\n",
       " 0.17848,\n",
       " 0.14708,\n",
       " 0.14736,\n",
       " 0.17354,\n",
       " 0.14392,\n",
       " 0.18106,\n",
       " 0.14798,\n",
       " 0.16372,\n",
       " 0.16164,\n",
       " 0.18952,\n",
       " 0.1407,\n",
       " 0.12446,\n",
       " 0.16218,\n",
       " 0.13792,\n",
       " 0.14276,\n",
       " 0.1688,\n",
       " 0.17418,\n",
       " 0.14536,\n",
       " 0.13248,\n",
       " 0.16142,\n",
       " 0.1479,\n",
       " 0.15428,\n",
       " 0.12814,\n",
       " 0.1665,\n",
       " 0.15724,\n",
       " 0.15594,\n",
       " 0.15658,\n",
       " 0.16544,\n",
       " 0.1493,\n",
       " 0.15432,\n",
       " 0.14576,\n",
       " 0.14832,\n",
       " 0.14054,\n",
       " 0.14246,\n",
       " 0.20478,\n",
       " 0.16328,\n",
       " 0.17044,\n",
       " 0.19378,\n",
       " 0.1938,\n",
       " 0.17016,\n",
       " 0.19384,\n",
       " 0.20238,\n",
       " 0.19934,\n",
       " 0.19434,\n",
       " 0.18718,\n",
       " 0.20292,\n",
       " 0.1841,\n",
       " 0.20676,\n",
       " 0.20584,\n",
       " 0.20724,\n",
       " 0.20274,\n",
       " 0.20274,\n",
       " 0.2056,\n",
       " 0.2074,\n",
       " 0.20312,\n",
       " 0.20724,\n",
       " 0.20724,\n",
       " 0.20754,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762,\n",
       " 0.20762]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_list_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03794,\n",
       " 0.0597,\n",
       " 0.05072,\n",
       " 0.04742,\n",
       " 0.04168,\n",
       " 0.05692,\n",
       " 0.0672,\n",
       " 0.04146,\n",
       " 0.0471,\n",
       " 0.04342,\n",
       " 0.03206,\n",
       " 0.04378,\n",
       " 0.0499,\n",
       " 0.06348,\n",
       " 0.0605,\n",
       " 0.04704,\n",
       " 0.04614,\n",
       " 0.0487,\n",
       " 0.02928,\n",
       " 0.0387,\n",
       " 0.02544,\n",
       " 0.05664,\n",
       " 0.05378,\n",
       " 0.04916,\n",
       " 0.05976,\n",
       " 0.05786,\n",
       " 0.0202,\n",
       " 0.0594,\n",
       " 0.03338,\n",
       " 0.04776,\n",
       " 0.02222,\n",
       " 0.051,\n",
       " 0.0363,\n",
       " 0.0241,\n",
       " 0.05196,\n",
       " 0.01338,\n",
       " 0.04158,\n",
       " 0.05868,\n",
       " 0.05204,\n",
       " 0.04512,\n",
       " 0.05216,\n",
       " 0.04538,\n",
       " 0.03134,\n",
       " 0.0442,\n",
       " 0.03752,\n",
       " 0.0442,\n",
       " 0.04348,\n",
       " 0.03712,\n",
       " 0.06002,\n",
       " 0.05576,\n",
       " 0.05394,\n",
       " 0.05056,\n",
       " 0.02662,\n",
       " 0.04802,\n",
       " 0.03182,\n",
       " 0.0499,\n",
       " 0.05414,\n",
       " 0.04022,\n",
       " 0.04376,\n",
       " 0.0468,\n",
       " 0.0441,\n",
       " 0.04786,\n",
       " 0.04866,\n",
       " 0.05352,\n",
       " 0.0501,\n",
       " 0.04454,\n",
       " 0.0348,\n",
       " 0.05096,\n",
       " 0.05394,\n",
       " 0.03748,\n",
       " 0.04434,\n",
       " 0.05504,\n",
       " 0.02448,\n",
       " 0.04516,\n",
       " 0.03356,\n",
       " 0.03516,\n",
       " 0.06098,\n",
       " 0.02636,\n",
       " 0.0433,\n",
       " 0.05018,\n",
       " 0.02576,\n",
       " 0.0295,\n",
       " 0.04816,\n",
       " 0.054,\n",
       " 0.04732,\n",
       " 0.06434,\n",
       " 0.05978,\n",
       " 0.054,\n",
       " 0.04542,\n",
       " 0.0326,\n",
       " 0.06282,\n",
       " 0.04294,\n",
       " 0.02538,\n",
       " 0.05846,\n",
       " 0.05638,\n",
       " 0.05848,\n",
       " 0.03962,\n",
       " 0.04912,\n",
       " 0.05458,\n",
       " 0.04892,\n",
       " 0.05078,\n",
       " 0.0344,\n",
       " 0.03406,\n",
       " 0.03358,\n",
       " 0.0448,\n",
       " 0.063,\n",
       " 0.05238,\n",
       " 0.04204,\n",
       " 0.0531,\n",
       " 0.0568,\n",
       " 0.04764,\n",
       " 0.06322,\n",
       " 0.0446,\n",
       " 0.0373,\n",
       " 0.03472,\n",
       " 0.06122,\n",
       " 0.03502,\n",
       " 0.0643,\n",
       " 0.06184,\n",
       " 0.03952,\n",
       " 0.02834,\n",
       " 0.03894,\n",
       " 0.04648,\n",
       " 0.04732,\n",
       " 0.04752,\n",
       " 0.0403,\n",
       " 0.06356,\n",
       " 0.04216,\n",
       " 0.0602,\n",
       " 0.0544,\n",
       " 0.04418,\n",
       " 0.04366,\n",
       " 0.04622,\n",
       " 0.05282,\n",
       " 0.03588,\n",
       " 0.05846,\n",
       " 0.05062,\n",
       " 0.03536,\n",
       " 0.05122,\n",
       " 0.03908,\n",
       " 0.0463,\n",
       " 0.03554,\n",
       " 0.03846,\n",
       " 0.05774,\n",
       " 0.056,\n",
       " 0.04906,\n",
       " 0.0383,\n",
       " 0.0364,\n",
       " 0.05068,\n",
       " 0.03776,\n",
       " 0.04638,\n",
       " 0.06414,\n",
       " 0.04058,\n",
       " 0.04948,\n",
       " 0.0709,\n",
       " 0.04134,\n",
       " 0.04958,\n",
       " 0.05236,\n",
       " 0.0556,\n",
       " 0.05654,\n",
       " 0.0275,\n",
       " 0.05008,\n",
       " 0.05586,\n",
       " 0.05608,\n",
       " 0.03364,\n",
       " 0.04284,\n",
       " 0.04754,\n",
       " 0.0485,\n",
       " 0.04412,\n",
       " 0.04538,\n",
       " 0.05384,\n",
       " 0.04346,\n",
       " 0.04602,\n",
       " 0.03624,\n",
       " 0.06666,\n",
       " 0.06514,\n",
       " 0.04668,\n",
       " 0.03054,\n",
       " 0.05346,\n",
       " 0.04144,\n",
       " 0.03812,\n",
       " 0.05096,\n",
       " 0.04846,\n",
       " 0.05268,\n",
       " 0.0461,\n",
       " 0.03896,\n",
       " 0.04998,\n",
       " 0.05476,\n",
       " 0.04742,\n",
       " 0.01894,\n",
       " 0.0465,\n",
       " 0.04332,\n",
       " 0.04108,\n",
       " 0.0634,\n",
       " 0.0449,\n",
       " 0.0501,\n",
       " 0.05812,\n",
       " 0.04774,\n",
       " 0.03586,\n",
       " 0.0478]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness_list_worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List has been saved to fitness_list_best.txt\n"
     ]
    }
   ],
   "source": [
    "file_name = \"fitness_list_best.txt\"\n",
    "\n",
    "# Saving the list to a text file\n",
    "with open(file_name, \"w\") as file:\n",
    "    for item in fitness_list_best:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "print(f\"List has been saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List has been saved to fitness_list_worst.txt\n"
     ]
    }
   ],
   "source": [
    "file_name = \"fitness_list_worst.txt\"\n",
    "\n",
    "# Saving the list to a text file\n",
    "with open(file_name, \"w\") as file:\n",
    "    for item in fitness_list_worst:\n",
    "        file.write(f\"{item}\\n\")\n",
    "\n",
    "print(f\"List has been saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
